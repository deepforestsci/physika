# ============================================================
# Fully Connected Network in Physika
# ============================================================

# ------------------------------------------------------------
# Activation: σ(x) = 1/(1 + e^(-x))
# ------------------------------------------------------------
def sigma(x: ℝ): ℝ:
    return 1.0 / (1.0 + exp(0.0 - x)) #TODO: suppoert negative values (-x = 0.0 - x)

# ============================================================
# Example 1: One-Layer Network (simple version)
# ============================================================
class OneLayerNet(W0: ℝ[2,3], c0: ℝ[2], w1: ℝ[2], b1: ℝ):
    def λ(x: ℝ[3]) → ℝ:
        return sigma(w1 @ sigma(W0 @ x + c0) + b1)
    def loss(y: ℝ, target: ℝ) → ℝ:
        return (y - target) ** 2.0

W0: ℝ[2,3] = [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6]]
c0: ℝ[2] = [0.1, 0.2]
w1: ℝ[2] = [0.7, 0.8]
b1: ℝ = 0.3
net1 = OneLayerNet(W0, c0, w1, b1)

net1([1.0, 2.0, 3.0])

# ============================================================
# Example 2: Generalized n-Layer Network
# ============================================================

class FullyConnectedNetwork(f: ℝ → ℝ, W: ℝ[2,3,3], B: ℝ[2,3], w: ℝ[3], b: ℝ, n: ℕ):
    def λ(x: ℝ[3]) → ℝ:
        for k in range(n):
            x = f(W[k] @ x + B[k])
        end
        return w @ x + b

    def loss(y: ℝ, target: ℝ) → ℝ:
        return (y - target) ** 2.0

# 2-layer network: ℝ[3] -> ℝ[3] -> ℝ[3] -> ℝ
W: ℝ[2,3,3] = [[[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]],
               [[0.2, 0.3, 0.4], [0.5, 0.6, 0.7], [0.8, 0.9, 0.1]]]
B: ℝ[2,3] = [[0.1, 0.2, 0.3], [0.1, 0.2, 0.3]]
w: ℝ[3] = [0.5, 0.5, 0.5]
b: ℝ = 0.1

net2 = FullyConnectedNetwork(sigma, W, B, w, b, 2)

net2([1.0, 2.0, 3.0])
net2([0.0, 0.0, 0.0])
net2([1.0, 1.0, 1.0])
